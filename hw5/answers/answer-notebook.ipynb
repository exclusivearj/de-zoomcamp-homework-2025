{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://edge.artifactory.ouroath.com:4443/artifactory/api/pypi/pypi-mirror/simple\n",
      "Requirement already satisfied: findspark in ./.venv/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: pyspark in ./.venv/lib/python3.9/site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in ./.venv/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_home = os.environ.get('SPARK_HOME', '/opt/homebrew/Cellar/apache-spark/3.5.4/libexec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.27:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkByExamples.com</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1329b53a0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 1\n",
    "\n",
    "v3.5.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-10-01 00:30:44|  2024-10-01 00:48:26|              1|          3.0|         1|                 N|         162|         246|           1|       18.4|  1.0|    0.5|       1.5|         0.0|                  1.0|        24.9|                 2.5|        0.0|\n",
      "|       1| 2024-10-01 00:12:20|  2024-10-01 00:25:25|              1|          2.2|         1|                 N|          48|         236|           1|       14.2|  3.5|    0.5|       3.8|         0.0|                  1.0|        23.0|                 2.5|        0.0|\n",
      "|       1| 2024-10-01 00:04:46|  2024-10-01 00:13:52|              1|          2.7|         1|                 N|         142|          24|           1|       13.5|  3.5|    0.5|       3.7|         0.0|                  1.0|        22.2|                 2.5|        0.0|\n",
      "|       1| 2024-10-01 00:12:10|  2024-10-01 00:23:01|              1|          3.1|         1|                 N|         233|          75|           1|       14.2|  3.5|    0.5|       2.0|         0.0|                  1.0|        21.2|                 2.5|        0.0|\n",
      "|       1| 2024-10-01 00:30:22|  2024-10-01 00:30:39|              1|          0.0|         1|                 N|         262|         262|           3|        3.0|  3.5|    0.5|       0.0|         0.0|                  1.0|         8.0|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_url = \"/Users/ajain17/Documents/Developer/de-zoomcamp-homework-2025/hw5/data/yellow_tripdata_2024-10.parquet\"\n",
    "df = spark.read.parquet(data_url)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ans2_df = df.repartition(4).write.mode(\"overwrite\").parquet(\"./hw5/answers/ans2-output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 194880\n",
      "-rw-r--r--@ 1 staff    24M Feb 26 21:00 part-00000-d1c9e3a4-55ee-45ba-a37c-1a00cad54754-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 staff    24M Feb 26 21:00 part-00001-d1c9e3a4-55ee-45ba-a37c-1a00cad54754-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 staff    24M Feb 26 21:00 part-00002-d1c9e3a4-55ee-45ba-a37c-1a00cad54754-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 staff    24M Feb 26 21:00 part-00003-d1c9e3a4-55ee-45ba-a37c-1a00cad54754-c000.snappy.parquet\n",
      "-rw-r--r--@ 1 staff     0B Feb 26 21:00 _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!ls -ltrgh ./hw5/answers/ans2-output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 2\n",
    "\n",
    "25MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128893"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 3\n",
    "# How many taxi trips were there on the 15th of October? Consider only trips that started on the 15th of October.\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "ans3 = df.filter(to_date(col(\"tpep_pickup_datetime\")) == \"2024-10-15\").count()\n",
    "ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 3\n",
    "\n",
    "128893\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+---------------------+-------------------+\n",
      "|PULocationID|DOLocationID|tpep_pickup_datetime|tpep_dropoff_datetime|trip_duration_hours|\n",
      "+------------+------------+--------------------+---------------------+-------------------+\n",
      "|          48|         265| 2024-10-16 13:03:49|  2024-10-23 07:40:53| 162.61777777777777|\n",
      "|         100|         100| 2024-10-03 18:47:25|  2024-10-09 18:06:55|            143.325|\n",
      "|         186|         226| 2024-10-22 16:00:55|  2024-10-28 09:46:33| 137.76055555555556|\n",
      "|         116|         163| 2024-10-18 09:53:32|  2024-10-23 04:43:37| 114.83472222222223|\n",
      "|         122|         138| 2024-10-21 00:36:24|  2024-10-24 18:30:18|  89.89833333333333|\n",
      "+------------+------------+--------------------+---------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "162.61777777777777"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 4: Longest trip\n",
    "# What is the length of the longest trip in the dataset in hours?\n",
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "trip_duration_df = df.withColumn(\"trip_duration_hours\", (unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))) / 3600)\n",
    "ans4_df = trip_duration_df.select(\"PULocationID\", \"DOLocationID\", \"tpep_pickup_datetime\",\"tpep_dropoff_datetime\", \"trip_duration_hours\").sort(col(\"trip_duration_hours\").desc())\n",
    "ans4_df.show(5)\n",
    "# ans4 = trip_duration_df.select(\"trip_duration_hours\").sort(col(\"trip_duration_hours\").desc()).first()[0]\n",
    "ans4 = ans4_df.select(\"trip_duration_hours\").first()[0]\n",
    "ans4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 4\n",
    "\n",
    "162\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 5\n",
    "\n",
    "Spark’s User Interface which shows the application's dashboard runs on which local port?\n",
    "\n",
    "`4040`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+-----+\n",
      "|pickup_zone                                  |count|\n",
      "+---------------------------------------------+-----+\n",
      "|Governor's Island/Ellis Island/Liberty Island|1    |\n",
      "|Rikers Island                                |2    |\n",
      "|Arden Heights                                |2    |\n",
      "|Jamaica Bay                                  |3    |\n",
      "|Green-Wood Cemetery                          |3    |\n",
      "+---------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Governor's Island/Ellis Island/Liberty Island\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 6: Least frequent pickup location zone\n",
    "# Using the zone lookup data and the Yellow October 2024 data, what is the name of the LEAST frequent pickup location Zone?\n",
    "\n",
    "# Load the zone lookup data into a temp view in Spark:\n",
    "taxi_zone_lookup_df = spark.read.csv(\"/Users/ajain17/Documents/Developer/de-zoomcamp-homework-2025/hw5/data/taxi_zone_lookup.csv\", header=True)\n",
    "taxi_zone_lookup_df.createOrReplaceTempView(\"taxi_zone_lookup\")\n",
    "# Temp view for yellow_tripdata\n",
    "df.createOrReplaceTempView(\"yellow_tripdata\")\n",
    "ans6_df = spark.sql(\"SELECT Zone as pickup_zone, count(*) as count FROM yellow_tripdata yt JOIN taxi_zone_lookup t ON yt.PULocationID = t.LocationID GROUP BY 1 ORDER BY count\")\n",
    "ans6_df.show(truncate=False, n=5)\n",
    "ans6 = ans6_df.select(\"pickup_zone\").first()[0]\n",
    "ans6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 6\n",
    "\n",
    "`Governor's Island/Ellis Island/Liberty Island`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
